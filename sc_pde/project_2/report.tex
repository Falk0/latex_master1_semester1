\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb}
\usepackage{natbib}
\usepackage{listings}
% figure support
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{pdfpages}
\usepackage{transparent}
\pdfsuppresswarningpagegroup=1

\title{Test title}
\author{Linus Falk}
\begin{document}
\maketitle
In this following problems we are examining the performance (iteration and run-time) of 4 algorithms for solving system of linear equations implemented in MATLAB: Jacobi method (jacobi.m), Gauss-Seidel method (gs.m), LU decomposition (Doolittle algorithm) (myownLU.m) and Conjugate gradient method (CG.m). The timing was done using the MATLAB script: Project\_2B.m  

\section*{Problem B1}
In this problem we are solving a small 4x4 system of lineare equations

\begin{equation}
\begin{aligned}
\textbf{A}= \begin{bmatrix} 10& -1& 2& 0\\-1& 11& -1& 3 \\2& -1& 10& -1 \\0& 3& -1& 8  \end{bmatrix}, \textbf{b}= \begin{bmatrix} 6 \\25 \\ -11\\ 15 \end{bmatrix}
\end{aligned}
\end{equation}

\begin{table}[ht!]
\centering
\begin{tabular}{lll}
 \textbf{Method}& \textbf{Iteration}  &\textbf{Time}\\ \hline
 Jacobi&  11& 0.004164 \\
 Gauss-Seidel&  6& 0.004300 \\ 
 CG& 9&  0.000377\\
myownLU& & 0.009256\\
Ab& & 0.000056\\
Matlab LU& & 0.000062\\\hline
\end{tabular}
\caption{Problem B1}
\label{tab:tab1}
\end{table}


We can observe that the MATLAB backslash function is the fastest of the selection of algorithms. This is because the backslash function is not only one algorithm but instead work by first examining the matrix and select method that is best suited for the problem. The MATLAB functions are also much better designed than the implementation of algorithms we constructed using the hardware much more efficiently.

\section*{Problem B2}

In this problem we are examining the run-time and number of iteration it takes to solve a randomly generated system of equations. A and b were generated with random numbers between 0 and 1. To the diagonal of A is extra "weight" added by changing the parameter w that adds w to all the diagonal entries of A. The test was then conducted for different sizes N of the systems 100,500 and 1000 and with different weights: 1,5,10 and 100. 

The convergence condition for any iterative method is when the spectral radius of the iteration matrix is less than 1 to guarantee convergence. The spectral radius of a square matrix is the maxiumum of the absolute values of the eigenvalues of the matrix. Spectral radius of the iterative matrix: 

\begin{equation}
\rho (D^{-1}(L+U)) < 1
\end{equation}

A condition for convergence is also that the matrix A is strictly diagonally dominant. This condition is sufficient for convergence but not necessary. Diagonal row dominant means:

\begin{equation}
|a_{ii}| > \sum_{j\neq i} |a_{ij}| 
\end{equation}



\begin{table}[ht!]
\centering
\begin{tabular}{lllll} 
\textbf{Method}& \textbf{Iterations}& \textbf{100}& \textbf{500}& \textbf{1000} \\ \hline
\textbf{Jacobi}& NaN/NaN/NaN& & &  \\ 
\textbf{Gauss-Seidel}& NaN/NaN/NaN& & & \\
\textbf{CG} &NaN/NaN/NaN & & & \\
\textbf{myownLU}& & & & \\ 
\textbf{Ab}& & 0.00023& 0.01010& 0.02241\\
\textbf{MATLAB LU}& & 0.00021& 0.00351& 0.02076 \\
 \hline

\end{tabular}
\caption{Problem B2, w = 1}
\label{tab:tab1}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{lllll}
\textbf{Method}& \textbf{Iterations}& \textbf{100}& \textbf{500}& \textbf{1000} \\ \hline
\textbf{Jacobi}& NaN/NaN/NaN& & & \\ 
\textbf{Gauss-Seidel}& NaN/NaN/NaN& & & \\
\textbf{CG}& NaN/NaN/NaN& & & \\
\textbf{myownLU}& & & & \\ 
\textbf{Ab}& & .00033& 0.00982& 0.01947\\
\textbf{MATLAB LU}& & 0.00020& 0.00936& 0.01541\\
 \hline

\end{tabular}
\caption{Problem B2, w = 5}
\label{tab:tab2}
\end{table}

\newpage
\begin{table}[ht!]
\centering
\begin{tabular}{lllll}
\textbf{Method}& \textbf{Iterations}& \textbf{100}& \textbf{500}& \textbf{1000} \\ \hline
\textbf{Jacobi}& NaN/NaN/NaN& & & \\ 
\textbf{Gauss-Seidel}& NaN/NaN/NaN& & & \\
\textbf{CG} & NaN/NaN/NaN& & &  \\
\textbf{myownLU}& & & & \\ 
\textbf{Ab}& & 0.00024& 0.009006& 0.02136\\
\textbf{MATLAB LU}& & 0.00036& 0.00426& 0.01410 \\
 \hline

\end{tabular}
\caption{Problem B2, w = 10}
\label{tab:tab3}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{lllll}
\textbf{Method}& \textbf{Iterations}& \textbf{100}& \textbf{500}& \textbf{1000} \\ \hline
\textbf{Jacobi}& NaN/NaN/NaN& & & \\ 
\textbf{Gauss-Seidel}& NaN/NaN/NaN& & & \\
\textbf{CG} & NaN/NaN/NaN& & &  \\
\textbf{myownLU} & & & & \\ 
\textbf{Ab}& & 0.00030& 0.00724& 0.02376\\
\textbf{MATLAB LU}& & 0.00025& 0.00534& 0.01787\\
 \hline

\end{tabular}
\caption{Problem B2, w = 100}
\label{tab:tab3}
\end{table}

\begin{table}[ht!]
\centering
\begin{tabular}{lllll}\textbf{N}&  $\rho$ (w = 1)&  $\rho$ (w = 5)&  $\rho$ (w = 10)&  $\rho$ (w = 100)\\ \hline

 100&  33.42&  9.15&  4.76&  0.49\\
 500&  171.81&  45.53&  23.26&  2.48\\
 1000&  346.61&  91.06&  47.63&   47.63\\ \hline
\end{tabular}
\caption{Spectral radius of iterative matrix}
\label{tab:tab1}
\end{table}

\newpage

When the spectral radius of the iterative matrix becomes less than 1 (and the matrix becomes Diagonal dominant row wise) at w=100 and N=100 we can observe that the Jacobi methods starts to work, this is because it guaranteed to solve systems with spectral radius < 1. 



Gauss-Siedel method is guaranteed to converge when the matrix is Diagonally dominant or for symmetric positive matrices. We can therefore not be certain that this method works for any other cases except for the same case that Jacobi method worked (N=100 and w = 100). Even though the it converges in many other cases we can see that the number of iterations are very high and that it would not be efficient to use it, and we cant be certain that it allways converges. 

The Conjugate gradient method shows similar behaviour as Gauss-Siedel, we can only prove that it converges for positive definite matrices so even if it converges it is not certain it would always do that and the number of iterations is often to high to be useful.


\newpage

\section*{Problem B3}
In this section is a large sparse matrix solved using the methods from earlier. When trying to solve this system some of the methods we can't really us: e.g: myownLU  was excluded, because $LU$ factorization mostly don't exists for random matrixes. They exists for symmetric positive definite matrixes so it wasn't a problem in Part B1. 

Even though the Conjugate Gradient method is very useful for large sparse matrices like this, it became clear that the improvement of each iteration for  $\alpha = 0,$ and $0.00001$  was very slow and the test was only finished with $\alpha =0.1 $ and $0.0001$. This is Because the improvement of each iteration of the Conjugate Gradient method depends on the condition number. The Conjugate gradient method would finish in the other cases also if we let it run and if the round off errors wouldnâ€™t become to big. 

For the other two methods (Jacobi and Gauss-Seidel) it was also clear that the condition number had an effect on speed of convergence, making it unpractical to time them for the ill conditioned systems where $\alpha = 0, 0.001$ and $0.00001$. 




\begin{itemize}
    \item for $\alpha = 0.1$  cond(A) $\approx $ 41
    \item for $\alpha = 0.001$  cond(A) $\approx $ 4000
    \item for $\alpha = 0.00001$  cond(A) $\approx $ 39600
    \item for $\alpha = 0$  cond(A) $\approx $ 4053700
\end{itemize}

\begin{table}[ht!]
\centering
\begin{tabular}{llllll}
\textbf{Method}& \textbf{Iteration}& a = 0 & a = 0.1 & a = 0.001 & a = 0.0001 \\ \hline
Jacobi& NaN/NaN/NaN/NaN  & & & &  \\
Gauss-Seidel& NaN/NaN/NaN/NaN  & & & &  \\ 
CG&  NaN/NaN/NaN/NaN & & & & \\
myownLU& & & & &\\
Ab& & 0.0003& 0.0003& 0.0002& 0.0004\\
Matlab LU& & 0.0014& 0.0016& 0.0014& 0.0017\\\hline
\end{tabular}
\caption{Result Problem B3}
\label{tab:tab1}
\end{table}


The result once again also highlight that even though Conjugate gradient method is a good method for this kind of problem are the MATLAB functions adaptive and much more efficiently constructed.



\bibliographystyle{unsrt}
\bibliography{references}
\end{document}