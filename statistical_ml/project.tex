\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb}
\usepackage{natbib}
\usepackage{float} 
\usepackage[caption = false]{subfig}
\usepackage{listings}
\lstset{
    breaklines=true,
    basicstyle=\tt\normalsize,
    keywordstyle=\color{blue},
    identifierstyle=\color{magenta},
    frame = single
} 
% figure support
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{pdfpages}
\usepackage{transparent}
\pdfsuppresswarningpagegroup=1

\title{SML project}
\author{Linus Falk}
\begin{document}
\maketitle
\section{Logistic regression}

Logistic regression is a classification model even though the same could suggest something different. 

\begin{equation}
z = \theta_0 + \theta_1x_1 + \theta_2x_2 + \ldots + \theta_px_p
\end{equation}

To fit our model to a prediction of probability $\text{p}(y = 1 | x) $ we use the logistic function also know as the Sigmoid function, defined as: 

\begin{equation}
f(\text{z}) = \frac{e^{z}} {1 + e^{z}} \in \begin{bmatrix} 0& 1 \end{bmatrix} 
\end{equation}


This implies in the classification problem with 2 classes that we have the probability or the model for the second class $\text{p}(y = -1 | x)$ : $ 1 - f(\text{z})$. Choosing 1 and -1 as labels simplify our expressions such that we are left with:

\begin{equation}
f(\text{z}) = \frac{e^{z}} {1 + e^{z}} = 1 - f(\text{z}) = \frac{e^{\theta^{T}x}} {1+e^{\theta^{T}x}}
\end{equation}



We want to find the parameters $\theta$ with the use of our training data. With the use the maximum likelihood approach we have:
\begin{equation}
\hat{\theta} = \text{arg max p(\textbf{y} $|$ \textbf{X;$\theta$})} = \text{arg max} \sum_{i=1}^{n} \text{ln } \text{p(y}_i | \textbf{x}_i;\theta)
\end{equation}

turning this into minimization problem by using the negative log likelihood as cost function and since we have chosen our labels in a clever way we end up with the cost function:

\begin{equation}
J(\theta) = \frac{1} {n} \sum_{i=1}^{n} \text{ln}(1+e^{-y_i\theta^{T}x_i})
\end{equation}

We now have the logistic regression model we need to find our parameters from (minimize the function):

\begin{equation}
\hat{\theta} = \text{arg min} \frac{1} {n} \sum_{i=1}^{n} \text{ln}(1+e^{-y_i\theta^{T}x_i})
\end{equation}

This modification of the regression model is not "perfect" since we must use numerical methods for finding the parameters since there is no closed expression for the cost. This can be solved numerically and we can choose different solvers which all have pros and cons

\begin{itemize}
    \item 
\end{itemize}

\begin{table}[ht!]
\centering
\begin{tabular}{lll}\hline
 &  &  \\
 &  &  \\
 &  & \\ \hline
\end{tabular}
\caption{example}
\label{tab:tab1}
\end{table}

\begin{itemize}
    \item 
\end{itemize}


\bibliographystyle{unsrt}
\bibliography{references}
\end{document}